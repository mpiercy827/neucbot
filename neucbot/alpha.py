import os
import re
import numpy
from operator import itemgetter

from neucbot import ensdf, utils

ALPHA_LIST_DIR = "./AlphaLists"
CHAIN_LIST_DIR = "./Chains"


class AlphaList:
    ALPHA_LIST_FILE_PATTERN = re.compile(
        r"^AlphaLists\/(?P<element>[A-Za-z]{1,2})(?P<isotope>\d{1,3})Alphas.dat"
    )

    def __init__(self, element, isotope):
        self.element = element
        self.isotope = isotope
        self.file_path = f"{ALPHA_LIST_DIR}/{self.element}{self.isotope}Alphas.dat"
        self.fetch_attempts = 3
        self.alphas = []

    @classmethod
    def from_filepath(cls, file_path):
        if alpha_file_match := cls.ALPHA_LIST_FILE_PATTERN.match(file_path):
            element = alpha_file_match.group("element")
            isotope = alpha_file_match.group("isotope")

            return cls(element, isotope)
        else:
            raise RuntimeError(f"Invalid file path for alphalist {file_path}")

    def load_or_fetch(self):
        while not os.path.isfile(self.file_path):
            if self.fetch_attempts < 0:
                raise RuntimeError(f"Unable to write alpha file to {self.file_path}")
            self.write()
            self.fetch_attempts -= 1

        return self.load()

    def load(self):
        file = open(self.file_path)

        # Parse alphalist files:
        # 1. Only parse lines that have 2+ tab-separated tokens
        # 2. Ignore any lines starting with "#"
        # 3. Return list of lists (where each sublist is a list of floats)
        self.alphas = [
            [float(token) for token in line.split()]  # Parse each token as float
            for line in file.readlines()  # for each line in file
            if line[0] != "#"
            and len(line.split()) >= 2  # except for lines matching these conditions
        ]

        file.close()

        return self.alphas

    def scale_by(self, branch_fraction):
        self.alphas = [
            [energy, intensity * branch_fraction] for [energy, intensity] in self.alphas
        ]

    def write(self):
        if os.path.exists(self.file_path):
            print(f"Alpha list file already exists at {self.file_path}")
        else:
            client = ensdf.Client(self.element, self.isotope)
            decay_file_text = client.read_or_fetch_decay_file()
            energyMaps = ensdf.Parser.parse(decay_file_text)
            file = open(self.file_path, "w")

            for energy, probability in energyMaps["alphas"].items():
                file.write(f"{str(energy/1000)}\t{probability}\n")

            file.close()

        return True

    # The condense function returns a modified list of alpha energy/intensity pairs,
    # generated by the following procedure:
    #   1. Sort the pairs by alpha energies (decreasing from highest to lowest)
    #   2. Compute the cumulative intensities, obtained by adding the intensities of
    #      the sorted alpha energies.
    #   3. Expand the list to fill in the gaps between alpha energies in increments
    #      of alpha_step_size. The cumulative intensity of
    #      (alpha_energy - n * alpha_step_size) is equal to the cumulative intensity
    #      of alpha_energy until the former expression is less than the next
    #      alpha_energy in the sorted list.
    def condense(self, alpha_step_size):
        sorted_alphas = sorted(self.alphas, key=itemgetter(0), reverse=True)

        cumulative_intensities = numpy.cumsum([alpha[1] for alpha in sorted_alphas])

        # Starting from (0 + step size) to (max sorted_alpha) in increments of alpha_step_size
        max_alpha = utils.round_half_up(sorted_alphas[0][0])
        alpha_steps = numpy.arange(max_alpha, 0, -alpha_step_size)

        alpha_index = 0
        condensed_alphas = []
        max_index = len(sorted_alphas) - 1

        for step in alpha_steps:
            # If the alpha step is LESS THAN the next alpha energy (rounded to 2
            # decimal places), increment alpha_index
            if alpha_index < max_index and step < utils.round_half_up(
                sorted_alphas[alpha_index + 1][0]
            ):
                alpha_index += 1

            intensity = cumulative_intensities[alpha_index]
            condensed_alphas.append([step, intensity])

        return condensed_alphas


class ChainAlphaList(AlphaList):
    CHAIN_LIST_FILE_PATTERN = re.compile(
        r"^Chains\/(?P<element>[A-Za-z]{1,2})(?P<isotope>\d{1,3})Chain.dat"
    )
    CHAIN_LIST_LINE_PATTERN = re.compile(
        r"^(?P<element>[A-Za-z]{1,2})(?P<isotope>\d{1,3})\s+(?P<branch_frac>[\d\.]+)$"
    )

    def __init__(self, element, isotope):
        self.element = element
        self.isotope = isotope
        self.file_path = f"{CHAIN_LIST_DIR}/{self.element}{self.isotope}Chain.dat"
        self._alpha_lists = []
        self.alphas = []

    @classmethod
    def from_filepath(cls, file_path):
        if chain_file_match := cls.CHAIN_LIST_FILE_PATTERN.match(file_path):
            element = chain_file_match.group("element")
            isotope = chain_file_match.group("isotope")

            return cls(element, isotope)
        else:
            raise RuntimeError(f"Invalid file path for chain alpha list {file_path}")

    def load_or_fetch(self):
        # Read in chain file list line by line, splitting into:
        #   - element symbol
        #   - mass_number
        #   - branching_ratio
        # load_or_fetch() alpha list for each symbol/mass_number pair
        # Scale intensity down by branching_ratio / 100
        file = open(self.file_path)

        for line in file.readlines():
            if match := self.CHAIN_LIST_LINE_PATTERN.match(line):
                branch_fraction = float(match.group("branch_frac")) / 100.0

                alpha_list = AlphaList(match.group("element"), match.group("isotope"))
                alpha_list.load_or_fetch()
                alpha_list.scale_by(branch_fraction)

                self._alpha_lists.append(alpha_list)
                self.alphas += alpha_list.alphas

        file.close()

        return self.alphas
